{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbmIPNpgh3rT",
        "outputId": "438b3fe7-20a8-4785-ee94-7e56b244fe8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLDHlowJh3rV"
      },
      "source": [
        "#  The minimal language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFXH0XTXtoPy"
      },
      "source": [
        "# Lets try with Shahnameh!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m_fNEkptreF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/theheroxx/shahnameh-minimal-LM/refs/heads/master/shahnameh.txt # Get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jZtybm3VtnaS"
      },
      "outputs": [],
      "source": [
        "# Read the Shahnameh text (UTF-8)\n",
        "shahnameh = open(\"shahnameh.txt\", encoding='utf-8').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F4UlKvAjt_H0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "sequence_length = 100\n",
        "\n",
        "def split_input(input_text, sequence_length):\n",
        "    for i in range(0, len(input_text), sequence_length):\n",
        "        yield input_text[i : i + sequence_length]\n",
        "\n",
        "features = list(split_input(shahnameh[:-1], sequence_length))\n",
        "labels = list(split_input(shahnameh[1:], sequence_length))\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "dataset = TextDataset(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evIWNrgHuGhZ",
        "outputId": "eb05727b-4669-4a6f-9ebc-0384ef99a8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "به نام خداو\n",
            "ه نام خداوند جان و خرد\n",
            "|کزین \n"
          ]
        }
      ],
      "source": [
        "x, y = dataset[0]\n",
        "print(x[1:12])\n",
        "print(y[1:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w9dgC2_wUPd",
        "outputId": "ed493110-5198-4653-fc06-7694be5184dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, sequence_length):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab = []\n",
        "        self.char_to_id = {}\n",
        "        self.id_to_char = {}\n",
        "    def adapt(self, texts):\n",
        "        chars = set()\n",
        "        for t in texts:\n",
        "            chars.update(list(t))\n",
        "        # Reserve 0 for padding and 1 for unknown\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + sorted(chars)\n",
        "        self.char_to_id = {c:i for i,c in enumerate(self.vocab)}\n",
        "        self.id_to_char = {i:c for c,i in self.char_to_id.items()}\n",
        "    def __call__(self, text):\n",
        "        ids = [self.char_to_id.get(c, self.char_to_id['<UNK>']) for c in text]\n",
        "        ids = ids[:self.sequence_length]\n",
        "        if len(ids) < self.sequence_length:\n",
        "            ids += [self.char_to_id['<PAD>']] * (self.sequence_length - len(ids))\n",
        "        return ids\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "    def vocabulary_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# Build tokenizer from dataset text features\n",
        "tokenizer = CharTokenizer(sequence_length)\n",
        "tokenizer.adapt([text for text, _ in dataset])\n",
        "\n",
        "\n",
        "vocabulary_size = tokenizer.vocabulary_size()\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iWTPoBHwY0S",
        "outputId": "998bf382-c078-4dcf-bd17-a1192cc2e3df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<PAD>',\n",
              " '<UNK>',\n",
              " '\\n',\n",
              " ' ',\n",
              " '(',\n",
              " ')',\n",
              " '|',\n",
              " '«',\n",
              " '»',\n",
              " '،',\n",
              " '؟',\n",
              " 'ء',\n",
              " 'آ',\n",
              " 'أ',\n",
              " 'ؤ',\n",
              " 'ئ',\n",
              " 'ا',\n",
              " 'ب',\n",
              " 'ت',\n",
              " 'ث',\n",
              " 'ج',\n",
              " 'ح',\n",
              " 'خ',\n",
              " 'د',\n",
              " 'ذ',\n",
              " 'ر',\n",
              " 'ز',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ص',\n",
              " 'ض',\n",
              " 'ط',\n",
              " 'ظ',\n",
              " 'ع',\n",
              " 'غ',\n",
              " 'ف',\n",
              " 'ق',\n",
              " 'ل',\n",
              " 'م',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ٔ',\n",
              " 'پ',\n",
              " 'چ',\n",
              " 'ژ',\n",
              " 'ک',\n",
              " 'گ',\n",
              " 'ی',\n",
              " '\\u200c']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocabulary() # full vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "o1Lm5-cVvhA7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, features, labels, tokenizer):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        f = torch.tensor(self.tokenizer(self.features[idx]), dtype=torch.long)\n",
        "        l = torch.tensor(self.tokenizer(self.labels[idx]), dtype=torch.long)\n",
        "        return f, l\n",
        "\n",
        "dataset = TokenizedDataset(features, labels, tokenizer)\n",
        "training_data = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUtMyYO1vhA7",
        "outputId": "399a9826-8c25-4dd5-e428-db1b7507e7da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 100]), torch.Size([64, 100]))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a, b = next(iter(training_data))  # a: [batch, seq_len], b: [batch, seq_len]\n",
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4j6Aiz8OvhA7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 256\n",
        "hidden_dim = 1024\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "model = CharRNN(vocabulary_size, embedding_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FnYejMNvhA7",
        "outputId": "fb88ab50-a446-4529-da3b-179837323219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (embedding): Embedding(50, 256)\n",
            "  (gru): GRU(256, 1024, batch_first=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=50, bias=True)\n",
            ")\n",
            "Total parameters: 4002354\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2PH1YF8vhA7",
        "outputId": "d1039f27-8816-492c-cbf5-4cbe006a183a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 loss: 1.6497 acc: 0.5100\n",
            "Epoch 2/20 loss: 1.3256 acc: 0.5952\n",
            "Epoch 3/20 loss: 1.2568 acc: 0.6143\n",
            "Epoch 4/20 loss: 1.2144 acc: 0.6264\n",
            "Epoch 5/20 loss: 1.1804 acc: 0.6361\n",
            "Epoch 6/20 loss: 1.1523 acc: 0.6442\n",
            "Epoch 7/20 loss: 1.1274 acc: 0.6512\n",
            "Epoch 8/20 loss: 1.1060 acc: 0.6576\n",
            "Epoch 9/20 loss: 1.0872 acc: 0.6628\n",
            "Epoch 10/20 loss: 1.0707 acc: 0.6675\n",
            "Epoch 11/20 loss: 1.0561 acc: 0.6715\n",
            "Epoch 12/20 loss: 1.0440 acc: 0.6751\n",
            "Epoch 13/20 loss: 1.0338 acc: 0.6779\n",
            "Epoch 14/20 loss: 1.0275 acc: 0.6799\n",
            "Epoch 15/20 loss: 1.0208 acc: 0.6813\n",
            "Epoch 16/20 loss: 1.0198 acc: 0.6819\n",
            "Epoch 17/20 loss: 1.0163 acc: 0.6826\n",
            "Epoch 18/20 loss: 1.0165 acc: 0.6826\n",
            "Epoch 19/20 loss: 1.0182 acc: 0.6819\n",
            "Epoch 20/20 loss: 1.0212 acc: 0.6810\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    correct = 0\n",
        "    for batch_x, batch_y in training_data:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(batch_x)\n",
        "        loss = criterion(logits.view(-1, vocabulary_size), batch_y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct += (preds == batch_y).float().sum().item()\n",
        "        total_tokens += batch_x.numel()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} loss: {total_loss/len(training_data.dataset):.4f} acc: {correct/total_tokens:.4f}\")\n",
        "\n",
        "# Save the model weights\n",
        "import os\n",
        "os.makedirs(\"/content/drive/MyDrive\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/shahnameh_model1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qKIF1HtaKie0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/shahnameh_tokenizer1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer.get_vocabulary(), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiD32FtwvhA7"
      },
      "source": [
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial test for making a generation model, a better code with GUI is available in the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7Mvecqm1vhA7"
      },
      "outputs": [],
      "source": [
        "# Create a generation-ready model (same architecture) and helper for stepwise generation\n",
        "generation_model = CharRNN(vocabulary_size, embedding_dim, hidden_dim)\n",
        "generation_model.load_state_dict(model.state_dict())\n",
        "generation_model.to(device)\n",
        "generation_model.eval()\n",
        "\n",
        "# Step function: given a token id and previous hidden, return logits for next token and new hidden\n",
        "import torch\n",
        "\n",
        "def generation_step(token_id, hidden):\n",
        "    x = torch.tensor([[token_id]], dtype=torch.long, device=device)  # batch=1, seq_len=1\n",
        "    with torch.no_grad():\n",
        "        logits, hidden = generation_model(x, hidden)\n",
        "    return logits[:, -1, :], hidden\n",
        "\n",
        "# Save the generation model weights\n",
        "torch.save(generation_model.state_dict(), \"/content/drive/MyDrive/shahnameh_generation_model1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RRncXtzvvhA8"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.get_vocabulary()\n",
        "char_to_id = {c:i for i,c in enumerate(tokens)}\n",
        "id_to_char = {i:c for c,i in char_to_id.items()}\n",
        "\n",
        "prompt = \"\"\"\n",
        "ضحاک\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "RqoIXHG6vhA8"
      },
      "outputs": [],
      "source": [
        "# Prepare prompt and initial hidden state\n",
        "input_ids = [char_to_id.get(c, char_to_id['<UNK>']) for c in prompt]\n",
        "hidden = torch.zeros(1, 1, hidden_dim, device=device)\n",
        "\n",
        "# Feed the prompt into the model to prime the hidden state\n",
        "with torch.no_grad():\n",
        "    for token_id in input_ids:\n",
        "        logits, hidden = generation_step(token_id, hidden)\n",
        "    # 'logits' now holds the distribution for the next char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsR9Hxa9vhA8"
      },
      "outputs": [],
      "source": [
        "generated_ids = []\n",
        "max_length = 250 # maximum generation tokens\n",
        "for i in range(max_length):\n",
        "    next_token = int(torch.argmax(logits, dim=-1).item()) # argmax sets the most possible word for each timestep to 1 and others to 0\n",
        "    generated_ids.append(next_token)\n",
        "    logits, hidden = generation_step(next_token, hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3u_MahevhA8",
        "outputId": "cecac35f-b982-4a0c-bdf0-78f9bc09649e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ضحاک\n",
            "|همی باش تا جاودان اندکی\n",
            "|بپرسید زو شهریار جهان\n",
            "|به پیش سپاه اندر آمد به جوش\n",
            "|ز هر سو برانگیخت اسفندیار\n",
            "|برفتند با نامداران را\n",
            "|بدو گفت بهرام کای شهریار\n",
            "|به مردی و نیک و بد و نیک رای\n",
            "|به مردی و گردی و گردنکشان\n",
            "|کنون چون بود رنج و آزار اوی\n",
            "|برو تا برآ\n"
          ]
        }
      ],
      "source": [
        "output = \"\".join([id_to_char[token_id] for token_id in generated_ids])\n",
        "print(prompt + output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
